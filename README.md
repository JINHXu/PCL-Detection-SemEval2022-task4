# SemEval2022-task4: PCL detection

Code repository for the [paper](): 

__Diana-Jinghua-Mourhaf at SemEval2022 Task 4: pre-BERT Neural Network Methods vs post-BERT RoBERTa Approach for Patronizing and Condescending Language Detection__


Authors: [Jinghua Xu](https://jinhxu.github.io/)

### shared-task

* [SemEval2022 Task 4: Patronizing and Condescending Language Detection](https://sites.google.com/view/pcl-detection-semeval2022/)

### abstract

> This paper describes my participation in the SemEval-2022 Task 4: Patronizing and Condescending Language Detection. I participate in both subtasks: Patronizing and Condescending Language (PCL) Identification and Patronizing and Condescending Language Categorization, with the main focus put on subtask 1. The experiments compare pre-BERT neural network (NN) based systems against post-BERT pretrained language model RoBERTa. This research finds NN-based systems in the experiments perform worse on the task compared to the pretrained language models. The top-performing RoBERTa system is ranked 27 out of 80 teams (F1-score: 54.64) in subtask 1, and 23 out of 50 teams (F1-score: 30.03) in subtask 2.

### code

* run `export PYTHONPATH="${PYTHONPATH}:path_to_wd/"` in terminal before running each script

### data

* upon request, [info](https://github.com/Perez-AlmendrosC/dontpatronizeme).
